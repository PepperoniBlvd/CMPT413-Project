{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Project Eval/Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:49:33.158732918Z",
     "start_time": "2024-04-12T05:49:33.107005993Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "import transformers\n",
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:47:40.954416704Z",
     "start_time": "2024-04-12T05:47:40.888954906Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define some utility functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "def load_model(model_name, checkpoint, tokenizer):\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = PeftModel.from_pretrained(model, checkpoint)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, max_length=150,\n",
    "                             padding=True, return_tensors=\"pt\").to(device)\n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T05:50:59.277870763Z",
     "start_time": "2024-04-12T05:50:59.233210824Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "checkpoint = \"checkpoint-11446\"  # assuming the google drive folder is downloaded in this directory\n",
    "file_name = None\n",
    "\n",
    "if any(k in model_name for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = load_model(model_name, checkpoint, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T05:47:49.692519228Z",
     "start_time": "2024-04-12T05:47:49.076583487Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc2ae8657bf5468893ad96e127e1022d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test some toy inputs and check the output of the pretrained model\n",
    "dataset_orig = {\n",
    "    \"premise\": [\"The cat is sitting on a mat\"], \n",
    "    \"hypothesis\": [\"The cat is sitting down\"]\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_orig)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"premise\", \"hypothesis\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T05:56:30.660709510Z",
     "start_time": "2024-04-12T05:56:30.610643390Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For premise 'The cat is sitting on a mat' and hypothesis 'The cat is sitting down', the predicted result is 'Entailment'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of correct predictions\n",
    "correct = torch.tensor(0, device=device)\n",
    "# Set the data needed\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "eval_dataloader = DataLoader(tokenized_datasets, batch_size=1, collate_fn=data_collator)\n",
    "# List of predictions to write to a file (if output file is given)\n",
    "ret = []\n",
    "total_len = 0\n",
    "# Evaluation loop\n",
    "for i, d in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "    f = d.to(device)\n",
    "    output = model(**f)\n",
    "    logits = output.logits\n",
    "    argmax = torch.argmax(logits, dim=1).float()\n",
    "    # If the labels are given, compute the test accuracy\n",
    "    if \"labels\" in f:\n",
    "        correct += torch.sum(f[\"labels\"] == argmax)\n",
    "    total_len += len(argmax)\n",
    "    ret += list(argmax.int().cpu().numpy())\n",
    "\n",
    "lab_map = {\n",
    "    0: \"Entailment\", 1: \"Neutral\", 2: \"Contradiction\"\n",
    "}\n",
    "for i in range(total_len):\n",
    "    print(f\"For premise '{dataset_orig['premise'][i]}' and hypothesis '{dataset_orig['hypothesis'][i]}', the predicted result is '{lab_map[ret[i]]}'.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T05:58:30.767832493Z",
     "start_time": "2024-04-12T05:58:30.664156783Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a simplified version of the training code, it is mostly used for testing the training code bit-by-bit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"snli\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:58:53.982614909Z",
     "start_time": "2024-04-12T05:58:41.720327385Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "if any(k in model_name for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=padding_side)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:58:56.565313886Z",
     "start_time": "2024-04-12T05:58:55.604737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", \"mnli\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:58:58.316483934Z",
     "start_time": "2024-04-12T05:58:57.210949139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label'],\n",
      "    num_rows: 200\n",
      "}) Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_percent = 0.1  # how much of the training data to train on?\n",
    "sz = int(len(dataset[\"train\"]) * train_percent)\n",
    "# Use a small subset of the training/testing data\n",
    "sz = 200\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "indices = np.random.default_rng().choice(len(dataset[\"train\"]),\n",
    "                                                 size=sz, replace=False)\n",
    "dataset[\"train\"] = dataset[\"train\"].select(indices)\n",
    "indices2 = np.random.default_rng().choice(len(dataset[\"test\"]),\n",
    "                                                 size=100, replace=False)\n",
    "dataset[\"test\"] = dataset[\"test\"].select(indices2)\n",
    "print(dataset[\"train\"], dataset[\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:58:59.889034107Z",
     "start_time": "2024-04-12T05:58:59.614975256Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00d2398bdc054259afc6f2b96b3a3d53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/200 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "806ac42ad0f1478ab99c3645e9b051b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, max_length=None, padding=True,return_tensors=\"pt\").to(device)\n",
    "    return outputs\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"premise\", \"hypothesis\"],\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:59:00.334211427Z",
     "start_time": "2024-04-12T05:59:00.183331784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,384 || all params: 81,963,264 || trainable%: 0.05903132408196921\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "prefix_tuning_config = peft.PrefixTuningConfig(\n",
    "    peft_type=\"PREFIX_TUNING\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    num_virtual_tokens=5,\n",
    "    prefix_projection=False, inference_mode=False)  # can turn on prefix project for a better result\n",
    "model = peft.get_peft_model(model, prefix_tuning_config)\n",
    "model.print_trainable_parameters()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:59:18.698604355Z",
     "start_time": "2024-04-12T05:59:18.299470869Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:59:19.212150621Z",
     "start_time": "2024-04-12T05:59:19.203689257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_models/gpt2-snli-finetune\",\n",
    "    disable_tqdm=False,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level = \"info\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-12T05:59:19.726039856Z",
     "start_time": "2024-04-12T05:59:19.696975518Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vboxuser/Documents/cmpt413/nlpclass-1241-g-new-milk-tarts/Project/venv/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 200\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14\n",
      "  Number of trainable parameters = 48,384\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/14 : < :, Epoch 0.14/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output_models/gpt2-snli-finetune/checkpoint-7\n",
      "loading configuration file config.json from cache at /home/vboxuser/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "tokenizer config file saved in output_models/gpt2-snli-finetune/checkpoint-7/tokenizer_config.json\n",
      "Special tokens file saved in output_models/gpt2-snli-finetune/checkpoint-7/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-12T05:59:20.579528110Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
